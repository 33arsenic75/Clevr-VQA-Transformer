import torch
import torch.nn as nn
import torchvision.models as models


class VQAModel(nn.Module):
    """
    A simple VQA model that combines image and text features using a transformer architecture.
    The model consists of:
    1. Image Encoder: A ResNet-101 model to extract image features.
    2. Text Encoder: A transformer encoder to process the question text.
    3. Cross Attention: A multi-head attention layer to combine image and text features.
    4. Classifier: A multi-layer perceptron (MLP) to predict the answer.
    """
    def __init__(self, vocab_size, num_classes, max_len=30, embed_dim=768):
        super(VQAModel, self).__init__()

        """=== Part 4: Image Encoder ==="""
        resnet = models.resnet101(pretrained=True)
        modules = list(resnet.children())[:-2]  # Remove avgpool and fc
        self.resnet = nn.Sequential(*modules)
        for param in self.resnet.parameters():
            param.requires_grad = False

        """linear projection to reduce feature dimension"""
        self.linear_proj = nn.Linear(2048, embed_dim)

        """=== Part 5: Text Encoder ==="""
        self.token_embed = nn.Embedding(vocab_size, embed_dim)
        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.randn(1, max_len + 1, embed_dim))  # +1 for CLS
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=8)
        self.text_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)

        """=== Part 6: Cross Attention ==="""
        self.cross_attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, batch_first=True)

        """=== Part 7: Classifier ==="""
        self.mlp = nn.Sequential(
            nn.Linear(embed_dim, 500),
            nn.ReLU(),
            nn.Linear(500, num_classes)
        )

    
    def forward(self, images, questions, attn_masks):
        """
        Forward pass through the model.
        Args:
            images: Tensor of shape [B, 3, H, W] (batch of images)
            questions: Tensor of shape [B, max_len] (batch of tokenized questions)
            attn_masks: Tensor of shape [B, max_len] (attention masks for questions)
        Returns:
            logits: Tensor of shape [B, num_classes] (predicted class scores)
        """
        """Extract image and text features, apply cross attention, and classify"""
        B = images.size(0)
        device = images.device
    
        """=== Image Features ==="""
        feat_map = self.resnet(images)  # [B, 2048, H, W]
        H, W = feat_map.size(2), feat_map.size(3)
        """feature map to 2D"""
        feat_map = feat_map.view(B, 2048, -1).permute(0, 2, 1)  # [B, H*W, 2048]
        img_feats = self.linear_proj(feat_map)  # [B, H*W, 768]
    
        """=== Text Features ==="""
        x = self.token_embed(questions)  # [B, max_len, 768]
        cls = self.cls_token.repeat(B, 1, 1)  # [B, 1, 768]
        x = torch.cat([cls, x], dim=1)  # [B, max_len+1, 768]
    
        """Use attn_masks from input (shape [B, max_len])"""
        pad_mask = (attn_masks == 0)
        pad_mask = torch.cat([torch.zeros(B, 1, dtype=torch.bool, device=device), pad_mask], dim=1)  # [B, L+1]
    
        """Permute x to [seq_len, batch, embed_dim] for transformer"""
        x = x.permute(1, 0, 2)  # [max_len+1, B, 768]
        x = x + self.pos_embed.permute(1, 0, 2)[:, :x.size(0), :]  # Adjust positional embedding
        x = self.text_encoder(x, src_key_padding_mask=pad_mask)  # [max_len+1, B, 768]
        x = x.permute(1, 0, 2)  # Back to [B, max_len+1, 768]
    
        cls_token_encoded = x[:, 0:1, :]  # [B, 1, 768]
    
        """=== Cross Attention ==="""
        attn_out, _ = self.cross_attention(
            query=cls_token_encoded,
            key=img_feats,
            value=img_feats
        )
    
        """=== Classifier ==="""
        logits = self.mlp(attn_out.squeeze(1))
        return logits
        
